{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The high-level idea is that the encoder could produce a representation of length equal to the original input sequence. Then, at decoding time, the decoder can (via some control mechanism) receive as input a context vector consisting of a weighted sum of the representations on the input at each time step. Intuitively, the weights determine the extent to which each step’s context “focuses” on each input token, and the key is to make this process for assigning the weights differentiable so that it can be learned along with all of the other neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 13:54:10.448519: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-12-26 13:54:10.448543: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-12-26 13:54:10.448547: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-12-26 13:54:10.448579: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-26 13:54:10.448594: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import util_functions as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores: (batch_size, num_queries_per_sample, num_keys)\n",
    "valid_lens: (batch_size, num_queries_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens, value=0):\n",
    "  \"\"\"\n",
    "  X: (batch_size, num_queries_per_sample, num_keys)\n",
    "  valid_lens: (batch_size, ) OR (batch_size, num_queries_per_sample)\n",
    "  \"\"\"\n",
    "  if valid_lens is None:\n",
    "    return tf.nn.softmax(logits=X, axis=-1)\n",
    "  \n",
    "  shape = X.shape\n",
    "  batch_size = shape[0]\n",
    "  num_queries_per_sample = shape[1]\n",
    "  num_keys = shape[2]\n",
    "\n",
    "  X = tf.reshape(X, shape=(-1, num_keys)) \n",
    "  ## Shape: (batch_size * num_queries_per_sample, num_keys)\n",
    "  if len(valid_lens.shape) == 1:\n",
    "    valid_lens = tf.repeat(valid_lens, repeats=shape[1]) \n",
    "    ## Shape (batch_size * num_queries_per_sample, )\n",
    "  else:\n",
    "    valid_lens = tf.reshape(valid_lens, shape=-1)       \n",
    "    ## Shape (batch_size * num_queries_per_sample, )\n",
    "\n",
    "  mask = tf.range(0, num_keys)      ## Shape (num_keys, )\n",
    "  mask = tf.expand_dims(mask, axis=0)     ## Shape (1, num_keys)\n",
    "  mask = tf.tile(mask, multiples=[batch_size*num_queries_per_sample, 1])     \n",
    "  ## Shape (batch_size * num_queries_per_sample, num_keys)\n",
    "\n",
    "  valid_lens = tf.expand_dims(valid_lens, axis=1) \n",
    "  ## Shape (batch_size * num_queries_per_sample, 1)\n",
    "\n",
    "  valid_lens = tf.tile(valid_lens, multiples=[1, num_keys])     \n",
    "  ## Shape (batch_size * num_queries_per_sample, num_keys)\n",
    "\n",
    "  masked_X = tf.where(mask < valid_lens, x=X, y=value)\n",
    "  masked_X = tf.reshape(masked_X, shape=shape)\n",
    "  return tf.nn.softmax(masked_X, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_softmax(tf.random.uniform(shape=(2, 2, 4)), tf.constant([2, 3]), value=-1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, dropout):\n",
    "    super().__init__()\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "  def call(self, queries, keys, values, valid_lens=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Queries: Decoder Input: (batch_size, num_queries_per_sample, dims)\n",
    "    Keys: Encoder Hidden States: (batch_size, num_keys, dims)           ## (num_keys = num_steps)\n",
    "    Values: Encoder Hidden States: (batch_size, num_keys, dims)         ## (num_keys = num_steps)\n",
    "    Valid Lens: (batch_size, ) OR (batch_size, num_queries_per_sample)  \n",
    "    \"\"\"\n",
    "    dims = queries.shape[-1]\n",
    "    scores = tf.matmul(a=queries, b=keys, transpose_b=True)/tf.math.sqrt(x=tf.cast(dims, dtype=tf.float32))\n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "    self.attention_weigts = masked_softmax(X=scores, valid_lens=valid_lens)\n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "\n",
    "    weights = self.dropout(self.attention_weigts, **kwargs) \n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "\n",
    "    ## (batch_size, num_queries, dims)\n",
    "    return tf.matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = tf.random.normal(shape=(2, 1, 2))\n",
    "keys = tf.random.normal(shape=(2, 10, 2))\n",
    "values = tf.random.normal(shape=(2, 10, 4))\n",
    "valid_lens = tf.constant([2, 6])\n",
    "\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention(queries, keys, values, valid_lens, training=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, key_dims, query_dims, num_hiddens, dropout, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    self.W_k = tf.keras.layers.Dense(units=num_hiddens, use_bias=False)\n",
    "    self.W_q = tf.keras.layers.Dense(units=num_hiddens, use_bias=False)\n",
    "    self.w_v = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "\n",
    "  def call(self, queries, keys, values, valid_lens=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Queries: Decoder Input: (batch_size, num_queries_per_sample, dims_1)\n",
    "    Keys: Encoder Hidden States: (batch_size, num_keys, dims_1)           ## (num_keys = num_steps)\n",
    "    Values: Encoder Hidden States: (batch_size, num_keys, dims)           ## (num_keys = num_steps)\n",
    "    Valid Lens: (batch_size, ) OR (batch_size, num_queries_per_sample)  \n",
    "    \"\"\"\n",
    "\n",
    "    queries = self.W_q(queries) # (batch_size, num_queries_per_sample, num_hiddens)\n",
    "    keys = self.W_k(keys)       # (batch_size, num_keys, num_hiddens)\n",
    "\n",
    "    features = tf.expand_dims(input=queries, axis=2) + tf.expand_dims(input=keys, axis=1)\n",
    "               # (batch_size, num_queries_per_sample, 1, num_hiddens) \n",
    "               # (batch_size, 1, num_keys, num_hiddens)\n",
    "               # Output: (batch_size, num_queries_per_sample, num_keys, num_hiddens)\n",
    "    features = tf.nn.tanh(features)\n",
    "    scores = tf.squeeze(self.w_v(features), axis=-1)\n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "    \n",
    "    self.attention_weigts = masked_softmax(X=scores, valid_lens=valid_lens)\n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "\n",
    "    weights = self.dropout(self.attention_weigts, **kwargs) \n",
    "    ## (batch_size, num_queries, num_keys)\n",
    "\n",
    "    ## (batch_size, num_queries, dims)\n",
    "    return tf.matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = tf.random.normal(shape=(2, 1, 20))\n",
    "\n",
    "attention = AdditiveAttention(key_dims=2, query_dims=20, num_hiddens=8,\n",
    "                              dropout=0.1)\n",
    "attention(queries, keys, values, valid_lens, training=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(utils.Decoder):\n",
    "  \"\"\"Base Attention Based Decoder Interface\"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  @property\n",
    "  def attention_weights(self):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers=2, dropout=0):\n",
    "    super().__init__()\n",
    "    self.attention = utils.AdditiveAttention(key_dims=num_hiddens, query_dims=num_hiddens, num_hiddens=num_hiddens, dropout=0)\n",
    "\n",
    "    ## accepts (num_steps, batch_size) as input\n",
    "    ## gives out (num_steps, batch_size, embed_size)\n",
    "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size)\n",
    "\n",
    "    ## Accepts (num_steps, batch_size, embed_size)\n",
    "    ## gives out (num_steps, batch_size, num_hiddens), [(batch_size, num_hiddens)]\n",
    "    self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells([\n",
    "      tf.keras.layers.GRUCell(num_hiddens, dropout=dropout) for _ in range(num_layers)\n",
    "    ]), return_sequences=True, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(units=vocab_size)\n",
    "    \n",
    "  def init_state(self, enc_all_outputs, *args):\n",
    "    \"\"\"returns encoder outs\"\"\"\n",
    "    return enc_all_outputs, *args\n",
    "  \n",
    "  def call(self, X, state):\n",
    "    \"\"\"\n",
    "    X: shape (batch_size, num_steps)\n",
    "    state: from encoder encoder outputs\n",
    "    Output: (num_steps, batch_size, vocab_size)\n",
    "    \"\"\"\n",
    "    num_steps = X.shape[1]\n",
    "    decoder_inputs = self.embedding(tf.transpose(X))\n",
    "    encoder_outputs, hidden_state, valid_lens = state\n",
    "    \n",
    "    context_vector = encoder_outputs[-1] \n",
    "    #(batch_size, num_hiddens)\n",
    "\n",
    "    context_vector = tf.expand_dims(input=context_vector, axis=0) \n",
    "    # (1, batch_size, num_hiddens)\n",
    "    \n",
    "    context_vector = tf.tile(input=context_vector, multiples=[num_steps, 1, 1]) \n",
    "    # (num_steps, batch_size, num_hiddens)\n",
    "\n",
    "    decoder_inputs = tf.concat([decoder_inputs, context_vector], axis=-1)\n",
    "\n",
    "    decoder_outputs, hidden_state = self.rnn(X=decoder_inputs, state=hidden_state)  \n",
    "    # (num_steps, batch_size, num_hiddens)\n",
    "    \n",
    "    outputs = self.dense(decoder_outputs) \n",
    "    # (num_steps, batch_size, vocab_size)\n",
    "\n",
    "    outputs = tf.transpose(outputs, perm=[1, 0, 2]) \n",
    "    # (batch_size, num_steps, vocab_size)\n",
    "\n",
    "    return outputs, [encoder_outputs, hidden_state]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

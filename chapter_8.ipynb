{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import util_functions as utils\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(utils.Classifier):\n",
    "  def __init__(self, lr=0.1, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4, activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Conv2D(filters=256, kernel_size=5, activation='relu', padding='same'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Conv2D(filters=384, kernel_size=3, activation='relu', padding='same'),\n",
    "      tf.keras.layers.Conv2D(filters=384, kernel_size=3, activation='relu', padding='same'),\n",
    "      tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(units=4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(units=4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(units=num_classes)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(d2l.Classifier):\n",
    "  def __init__(self, lr=0.1, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=96, kernel_size=11, strides=4, activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Conv2D(filters=256, kernel_size=5, padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),\n",
    "      tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same', activation='relu'),\n",
    "      tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(4096, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(num_classes)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet().layer_summary(X_shape=(1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "  data = utils.FashionMNISTData(batch_size=128, resize=(224, 224))\n",
    "  trainer = utils.Trainer(max_epochs=3)\n",
    "  model = AlexNet(lr=0.1, num_classes=10)\n",
    "  trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.board.data['train_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, num_channels):\n",
    "  blk = tf.keras.models.Sequential()\n",
    "  for _ in range(num_convs):\n",
    "    blk.add(tf.keras.layers.Conv2D(filters=num_channels, kernel_size=3, padding='same', activation='relu'))\n",
    "  blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "  return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(utils.Classifier):\n",
    "  def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential()\n",
    "    for (num_convs, num_channels) in arch:\n",
    "      self.net.add(vgg_block(num_convs, num_channels))\n",
    "    self.net.add(\n",
    "      tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "      ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGNet(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(X_shape=(1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "  data = utils.FashionMNISTData(batch_size=128, resize=(224, 224))\n",
    "  trainer = utils.Trainer(max_epochs=10)\n",
    "  model = VGGNet(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)))\n",
    "  trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network in Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(out_channels, kernel_size, strides, padding):\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=out_channels, kernel_size=kernel_size, strides=strides, padding=padding, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=out_channels, kernel_size=1, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=out_channels, kernel_size=1, activation='relu'),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiN(utils.Classifier):\n",
    "  def __init__(self, lr=0.1, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential([\n",
    "      nin_block(out_channels=96, kernel_size=11, strides=4, padding='valid'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      nin_block(out_channels=256, kernel_size=5, strides=1, padding='same'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      nin_block(out_channels=384, kernel_size=3, strides=1, padding='same'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      nin_block(out_channels=num_classes, kernel_size=3, strides=1, padding='same'),\n",
    "      tf.keras.layers.GlobalAvgPool2D(),\n",
    "      tf.keras.layers.Flatten()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NiN().layer_summary(X_shape=(1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "  data = utils.FashionMNISTData(batch_size=128, resize=(224, 224))\n",
    "  trainer = utils.Trainer(max_epochs=10)\n",
    "  model = NiN()\n",
    "  trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet (Inception Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(tf.keras.Model):\n",
    "  def __init__(self, c1, c2, c3, c4):\n",
    "    super().__init__()\n",
    "    self.b1_1 = tf.keras.layers.Conv2D(filters=c1, kernel_size=1, activation='relu')\n",
    "\n",
    "    self.b2_1 = tf.keras.layers.Conv2D(filters=c2[0], kernel_size=1, activation='relu')\n",
    "    self.b2_2 = tf.keras.layers.Conv2D(filters=c2[1], kernel_size=3, activation='relu', padding='same')\n",
    "\n",
    "    self.b3_1 = tf.keras.layers.Conv2D(filters=c3[0], kernel_size=1, activation='relu')\n",
    "    self.b3_2 = tf.keras.layers.Conv2D(filters=c3[1], kernel_size=5, activation='relu', padding='same')\n",
    "\n",
    "    self.b4_1 = tf.keras.layers.MaxPool2D(pool_size=3, strides=1, padding='same')\n",
    "    self.b4_2 = tf.keras.layers.Conv2D(filters=c4, kernel_size=1, activation='relu')\n",
    "\n",
    "  def call(self, x):\n",
    "    b1 = self.b1_1(x)\n",
    "    b2 = self.b2_2(self.b2_1(x))\n",
    "    b3 = self.b3_2(self.b3_1(x))\n",
    "    b4 = self.b4_2(self.b4_1(x))\n",
    "    return tf.keras.layers.Concatenate()([b1, b2, b3, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(utils.Classifier):\n",
    "  def b1(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same') ## dim/4, dim/4, 64\n",
    "    ])\n",
    "  \n",
    "  def b2(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=64, kernel_size=1, activation='relu'),\n",
    "      tf.keras.layers.Conv2D(filters=192, kernel_size=3, padding='same', activation='relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same') ## dim/8, dim/8, 192\n",
    "    ])\n",
    "  \n",
    "  def b3(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      Inception(c1=64, c2=(96, 128), c3=(16, 32), c4=32), ## Output Channels: 64 + 128 + 32 + 32\n",
    "      Inception(c1=128, c2=(128, 192), c3=(32, 96), c4=64), ## Output Channels: 128 + 192 + 96 + 64\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same') ## dim/16, dim/16, 480\n",
    "    ])\n",
    "  \n",
    "  def b4(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      Inception(c1=192, c2=(96, 208), c3=(16, 48), c4=64), \n",
    "      Inception(c1=160, c2=(112, 224), c3=(24, 64), c4=64),\n",
    "      Inception(c1=128, c2=(128, 256), c3=(24, 64), c4=64),\n",
    "      Inception(c1=112, c2=(144, 288), c3=(32, 64), c4=64),\n",
    "      Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same') ## dim/32, dim/32, 832\n",
    "    ])\n",
    "  \n",
    "  def b5(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      Inception(c1=256, c2=(160, 320), c3=(24, 128), c4=128),\n",
    "      Inception(c1=384, c2=(192, 384), c3=(48, 128), c4=128), ## dim/32, dim/32, 1024\n",
    "      tf.keras.layers.GlobalAvgPool2D(), ## 1024\n",
    "      tf.keras.layers.Flatten()\n",
    "    ])\n",
    "  \n",
    "  def __init__(self, lr=0.1, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.Sequential([\n",
    "      self.b1(), self.b2(), self.b3(), self.b4(), self.b5(),\n",
    "      tf.keras.layers.Dense(num_classes)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogleNet().layer_summary((1, 96, 96, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm and Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Norm\n",
    "  - improve convergence of NNs\n",
    "  - Batch normalization is applied to individual layers, or optionally, to all of them: In each training iteration, we first normalize the inputs (of batch normalization) by subtracting their mean and dividing by their standard deviation, where both are estimated based on the statistics of the current minibatch. Next, we apply a scale coefficient and an offset to recover the lost degrees of freedom. It is precisely due to this normalization based on batch statistics that batch normalization derives its name.\n",
    "  - once the model is trained, we can calculate the means and variances of each layer’s variables based on the entire dataset. Indeed this is standard practice for models employing batch normalization; thus batch normalization layers function differently in training mode (normalizing by minibatch statistics) than in prediction mode (normalizing by dataset statistics). In this form they closely resemble the behavior of dropout regularization of Section 5.6, where noise is only injected during training\n",
    "  - Dense Layers: Wx + b has dims (n_samples, n_units)\n",
    "    Compute mean and variance dims (1, n_units)\n",
    "  - Conv Layers: Output of Conv Layer (n_samples, n_width, n_height, n_channels)\n",
    "    Compute mean and variance over n_samples, n_width & n_height: (1, 1, 1, n_channels)\n",
    "  \n",
    "Layer Norm:\n",
    "  - normalization within an example\n",
    "  - Dense Layers: Wx + b has dims (n_samples, n_units)\n",
    "    Compute mean and variance dims across n_units (n_samples, 1)\n",
    "  - Conv Layers: Conv Layer (n_samples, n_width, n_height, n_channels)\n",
    "    Compute mean and variance over n_width & n_height: (n_samples, 1, 1, n_channels)\n",
    "\n",
    "Implementation Details:\n",
    "Putting aside the algorithmic details, note the design pattern underlying our implementation of the layer. Typically, we define the mathematics in a separate function, say batch_norm. We then integrate this functionality into a custom layer, whose code mostly addresses bookkeeping matters, such as moving data to the right device context, allocating and initializing any required variables, keeping track of moving averages (here for mean and variance), and so on. This pattern enables a clean separation of mathematics from boilerplate code. Also note that for the sake of convenience we did not worry about automatically inferring the input shape here; thus we need to specify the number of features throughout. By now all modern deep learning frameworks offer automatic detection of size and shape in the high-level batch normalization APIs (in practice we will use this instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import util_functions as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps):\n",
    "  inv = tf.cast(tf.math.rsqrt(x=moving_var + eps), dtype=X.dtype)\n",
    "  inv *= gamma\n",
    "  Y = inv * (X - moving_mean) + beta\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(BatchNorm, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    weight_shape = [input_shape[-1], ]\n",
    "    self.gamma = self.add_weight(name='gamma', shape=weight_shape, initializer=tf.ones_initializer(), trainable=True)\n",
    "    self.beta = self.add_weight(name='beta', shape=weight_shape, initializer=tf.zeros_initializer(), trainable=True)\n",
    "    self.moving_mean = self.add_weight(name='moving_mean', shape=weight_shape, initializer=tf.zeros_initializer(), trainable=False)\n",
    "    self.moving_var = self.add_weight(name='moving_var', shape=weight_shape, initializer=tf.ones_initializer(), trainable=False)\n",
    "    super(BatchNorm, self).build(input_shape)\n",
    "\n",
    "  def assign_moving_average(self, variable, value):\n",
    "    momentum = 0.1\n",
    "    delta = (1.0 - momentum) * variable + momentum * value\n",
    "    return variable.assign(delta)\n",
    "  \n",
    "  @tf.function\n",
    "  def call(self, inputs, training):\n",
    "    if training:\n",
    "      axis = list(range(len(inputs) - 1))\n",
    "      batch_mean = tf.reduce_mean(input_tensor=inputs, axis=axis, keepdims=True)\n",
    "      batch_var = tf.reduce_mean(tf.math.squared_difference(inputs, tf.stop_gradient(batch_mean)), axis=axis, keepdims=True)\n",
    "      mean_update = self.assign_moving_average(self.moving_mean, batch_mean)\n",
    "      variance_update = self.assign_moving_average(self.moving_var, value=batch_var)\n",
    "      self.add_update(mean_update)\n",
    "      self.add_update(variance_update)\n",
    "      mean, variance = batch_mean, batch_var\n",
    "    else:\n",
    "      mean, variance = self.moving_mean, self.moving_var\n",
    "\n",
    "    output = batch_norm(X=inputs, gamma=self.gamma, beta=self.beta, moving_mean=mean, moving_var=variance, eps=1e-5)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(utils.Classifier):\n",
    "  def __init__(self, lr=0.01, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=6, kernel_size=5, padding=\"same\", strides=1),\n",
    "      BatchNorm(),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      ## Output: (28, 28, 6) Params: (5, 5, 3, 6) + 6\n",
    "      tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n",
    "      ## Output: (14, 14, 6)\n",
    "      tf.keras.layers.Conv2D(filters=16, kernel_size=5, padding=\"valid\", strides=1),\n",
    "      BatchNorm(),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      ## Output: (10, 10, 16) Params: (5, 5, 6, 16) + 16\n",
    "      tf.keras.layers.AvgPool2D(pool_size=2, strides=2),\n",
    "      ## Output: (5, 5, 16)\n",
    "      tf.keras.layers.Flatten(),\n",
    "      ## Output: (400)\n",
    "      tf.keras.layers.Dense(units=120),\n",
    "      BatchNorm(),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      ## Output: (120) Params: (400, 120) + 120\n",
    "      tf.keras.layers.Dense(units=84),\n",
    "      BatchNorm(),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      ## Output: (84) Params: (120, 84) + 84\n",
    "      tf.keras.layers.Dense(units=10),\n",
    "      ## Output: (10) Params: (84, 10) + 10\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(lr=0.1)\n",
    "model.layer_summary(X_shape=(1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "  data = utils.FashionMNISTData(batch_size=128, resize=(28, 28))\n",
    "  trainer = utils.Trainer(max_epochs=10)\n",
    "  model = LeNet(lr=0.1)\n",
    "  trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(tf.keras.Model):\n",
    "  def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "    super().__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(filters=num_channels, kernel_size=3, padding='same', strides=strides)\n",
    "    self.conv2 = tf.keras.layers.Conv2D(filters=num_channels, kernel_size=3, padding='same', strides=1)\n",
    "    self.conv3 = None\n",
    "    if use_1x1conv:\n",
    "      self.conv3 = tf.keras.layers.Conv2D(filters=num_channels, kernel_size=1, padding='same', strides=strides)\n",
    "    \n",
    "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "    self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, X):\n",
    "    Y = tf.keras.activations.relu(self.bn1(self.conv1(X)))\n",
    "    Y = self.bn2(self.conv2(Y))\n",
    "    if self.conv3 is not None:\n",
    "      X = self.conv3(X)\n",
    "    Y += X\n",
    "    return tf.keras.activations.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal(shape=(4, 6, 6, 3))\n",
    "blk = Residual(6, use_1x1conv=True, strides=2)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(utils.Classifier):\n",
    "  def b1(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding='same'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "    ])\n",
    "  \n",
    "  def block(self, num_residuals, num_channels, first_block=False):\n",
    "    blk = tf.keras.models.Sequential()\n",
    "    for i in range(num_residuals):\n",
    "      if i == 0 and not first_block:\n",
    "        blk.add(Residual(num_channels=num_channels, use_1x1conv=True, strides=2))\n",
    "      else:\n",
    "        blk.add(Residual(num_channels=num_channels))\n",
    "    return blk\n",
    "\n",
    "  def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential(self.b1())\n",
    "    for i, b in enumerate(arch):\n",
    "      num_residual, num_channels = b\n",
    "      self.net.add(self.block(num_residuals=num_residual, num_channels=num_channels, first_block=(i==0)))\n",
    "    self.net.add(tf.keras.models.Sequential([\n",
    "      tf.keras.layers.GlobalAvgPool2D(),\n",
    "      tf.keras.layers.Dense(units=num_classes)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(ResNet):\n",
    "  def __init__(self, lr=0.1, num_classes=10):\n",
    "    super().__init__(arch=((2, 64), (2, 128), (2, 256), (2, 512)), lr=lr, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18().layer_summary(X_shape=(1, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, b in enumerate(((2, 64), (2, 128), (2, 256), (2, 512))):\n",
    "  print(i)\n",
    "  num_residual, num_channels = b\n",
    "  print(num_residual, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNexT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import util_functions as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_channels):\n",
    "    super(ConvBlock, self).__init__()\n",
    "    self.bn = tf.keras.layers.BatchNormalization()\n",
    "    self.relu = tf.keras.layers.ReLU()\n",
    "    self.conv = tf.keras.layers.Conv2D(filters=num_channels, kernel_size=(3, 3), padding='same')\n",
    "    self.listLayers = [self.bn, self.relu, self.conv]\n",
    "\n",
    "  def call(self, x):\n",
    "    y = x\n",
    "    for layer in self.listLayers.layers:\n",
    "      y = layer(y)\n",
    "    y = tf.keras.layers.concatenate([x, y], axis=-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_convs, num_channels):\n",
    "    super(DenseBlock, self).__init__()\n",
    "    self.listLayers = []\n",
    "    for _ in range(num_convs):\n",
    "      self.listLayers.append(ConvBlock(num_channels=num_channels))\n",
    "\n",
    "  def call(self, x):\n",
    "    for layer in self.listLayers.layers:\n",
    "      x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 8, 23])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(num_convs=2, num_channels=10)\n",
    "X = tf.random.uniform(shape=(4, 8, 8, 3))\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_channels, **kwargs):\n",
    "    super(TransitionBlock, self).__init__(**kwargs)\n",
    "    self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    self.relu = tf.keras.layers.ReLU()\n",
    "    self.conv = tf.keras.layers.Conv2D(filters=num_channels, kernel_size=1)\n",
    "    self.avg_pool = tf.keras.layers.AvgPool2D(pool_size=2, strides=2)\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.batch_norm(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.conv(x)\n",
    "    return self.avg_pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 4, 4, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = TransitionBlock(num_channels=10)\n",
    "X = tf.random.uniform(shape=(4, 8, 8, 3))\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(utils.Classifier):\n",
    "  def b1(self):\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding='same'),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.ReLU(),\n",
    "      tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')\n",
    "    ])\n",
    "  \n",
    "  def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4), lr=0.1, num_classes=10):\n",
    "    super(DenseNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = tf.keras.models.Sequential(self.b1())\n",
    "    for i, num_convs in enumerate(arch):\n",
    "      self.net.add(DenseBlock(num_convs=num_convs, num_channels=growth_rate))\n",
    "      num_channels += num_convs*growth_rate\n",
    "      if i == len(arch) - 1:\n",
    "        continue\n",
    "      else:\n",
    "        num_channels //=2\n",
    "        self.net.add(TransitionBlock(num_channels=num_channels))\n",
    "    \n",
    "    self.net.add(tf.keras.models.Sequential([\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.ReLU(),\n",
    "      tf.keras.layers.GlobalAvgPool2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(units=num_classes)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t (1, 56, 56, 64)\n",
      "DenseBlock output shape:\t (1, 56, 56, 192)\n",
      "TransitionBlock output shape:\t (1, 28, 28, 96)\n",
      "DenseBlock output shape:\t (1, 28, 28, 224)\n",
      "TransitionBlock output shape:\t (1, 14, 14, 112)\n",
      "DenseBlock output shape:\t (1, 14, 14, 240)\n",
      "TransitionBlock output shape:\t (1, 7, 7, 120)\n",
      "DenseBlock output shape:\t (1, 7, 7, 248)\n",
      "Sequential output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "DenseNet().layer_summary(X_shape=(1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
